
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="X-UA-Compatible" content="IE=Edge" />
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>Installation overview &#8212; Omnivector Slurm Distribution 0.0.1 documentation</title>
    <link rel="stylesheet" href="../_static/alabaster.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script async="async" type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="OSD Architecture" href="../architecture.html" />
    <link rel="prev" title="Welcome to the documentation for the Omnivector Slurm Distribution!" href="../index.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="installation-overview">
<span id="installation"></span><h1>Installation overview<a class="headerlink" href="#installation-overview" title="Permalink to this headline">¶</a></h1>
<p>The OSD can be installed on a cloud of your choosing. The only common component
used across deployments is <a class="reference external" href="https://juju.is">juju</a> itself.  You must have the juju client
installed on your local system in order to administer the installation and
deployment of the Slurm charms.</p>
<div class="section" id="install-juju">
<h2>Install Juju<a class="headerlink" href="#install-juju" title="Permalink to this headline">¶</a></h2>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ sudo snap install juju --classic
</pre></div>
</div>
<p>Once the juju client is installed you will be ready to proceed with deploying
Slurm to a cloud of your choosing.</p>
</div>
<div class="section" id="setup-cloud">
<h2>Setup cloud<a class="headerlink" href="#setup-cloud" title="Permalink to this headline">¶</a></h2>
<p>Follow the documentation for the cloud you with to deploy Slurm on:</p>
<div class="section" id="aws">
<h3>AWS<a class="headerlink" href="#aws" title="Permalink to this headline">¶</a></h3>
<div class="section" id="login-to-a-juju-controller">
<h4>Login to a juju controller<a class="headerlink" href="#login-to-a-juju-controller" title="Permalink to this headline">¶</a></h4>
<p>In any deployment scenario we will need to be logged into a juju controller.
For this example we will login to the public juju controller, Jaas.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ juju login jaas
</pre></div>
</div>
</div>
<div class="section" id="add-a-model">
<h4>Add a model<a class="headerlink" href="#add-a-model" title="Permalink to this headline">¶</a></h4>
<p>Once you are logged into a juju controller you need to add a model. Run the
following command to add the model that will house the OSD.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ juju add-model slurm aws/us-west-2
</pre></div>
</div>
</div>
</div>
<div class="section" id="lxd">
<h3>LXD<a class="headerlink" href="#lxd" title="Permalink to this headline">¶</a></h3>
<div class="section" id="bootstrap-a-localhost-lxd-juju-controller">
<h4>Bootstrap a localhost LXD Juju controller<a class="headerlink" href="#bootstrap-a-localhost-lxd-juju-controller" title="Permalink to this headline">¶</a></h4>
<p>In any deployment scenario we will need to be logged into a juju controller.
For this example we will bootstrap a juju controller in a LXD container on our
local machine.</p>
<p>Install and configure LXD, if you haven't already:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ sudo snap install lxd
$ lxd init --auto
$ lxd network <span class="nb">set</span> lxdbr0 ipv6.address none
</pre></div>
</div>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Juju does not support IPv6, the last command disables it.</p>
</div>
<p>You can now bootstrap your local cloud:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ juju bootstrap localhost
</pre></div>
</div>
<p>Following a successful bootstrap, <code class="docutils literal notranslate"><span class="pre">juju</span> <span class="pre">controllers</span></code> will show your newly
provisioned lxd controller.</p>
</div>
<div class="section" id="id1">
<h4>Add a model<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h4>
<p>Once you have created your juju controller you need to add a model. Run the
following command to add the model that will house the OSD.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ juju add-model slurm
</pre></div>
</div>
</div>
</div>
<div class="section" id="maas">
<h3>MAAS<a class="headerlink" href="#maas" title="Permalink to this headline">¶</a></h3>
<div class="section" id="id2">
<h4>Login to a juju controller<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h4>
<p>If you don't already have a juju controller, bootstrap juju by creating a juju
controller machine.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ juju bootstrap
</pre></div>
</div>
</div>
<div class="section" id="id3">
<h4>Add a model<a class="headerlink" href="#id3" title="Permalink to this headline">¶</a></h4>
<p>Once you are logged into a juju controller you need to add a model. Run the
following command to add the model that will house the OSD.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ juju add-model slurm
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="deploy-slurm">
<h2>Deploy Slurm<a class="headerlink" href="#deploy-slurm" title="Permalink to this headline">¶</a></h2>
<p>Now it is time to get Slurm :)</p>
<p>First we need to build a <em>charm</em>. That's what Juju will deploy to our cloud.
Charms are built with <code class="docutils literal notranslate"><span class="pre">charmcraft</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ sudo snap install --edge charmcraft
</pre></div>
</div>
<p>Clone the <a class="reference external" href="https://github.com/omnivector-solutions/slurm-charms">slurm-charms</a>
git repository, it contains all the nuts and bolts to build the charms:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ git clone https://github.com/omnivector-solutions/slurm-charms
$ <span class="nb">cd</span> slurm-charms
$ make charms
</pre></div>
</div>
<p>Now it is time to deploy! The bundles and overlays are in a separate repository,
<a class="reference external" href="https://github.com/omnivector-solutions/slurm-bundles">slurm-bundles</a>.
Clone the repository on the same directory you cloned <code class="docutils literal notranslate"><span class="pre">slurm-charms/</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ git clone https://github.com/omnivector-solutions/slurm-bundles
$ <span class="nb">cd</span> slurm-bundles
</pre></div>
</div>
<p>The <code class="docutils literal notranslate"><span class="pre">slurm-core</span></code> directory contains all the bundles and overlays to deploy a
basic Slurm cluster:</p>
<ul class="simple">
<li><code class="docutils literal notranslate"><span class="pre">slurm-core/bundle.yaml</span></code>: the basic definition of the Slurm components.</li>
<li><code class="docutils literal notranslate"><span class="pre">slurm-core/clouds/</span></code>: overlays with specific settings for each supported
cloud environment.</li>
<li><code class="docutils literal notranslate"><span class="pre">slurm-core/options/</span></code>: overlays with specific <code class="docutils literal notranslate"><span class="pre">options</span></code> for the Slurm
components.</li>
<li><code class="docutils literal notranslate"><span class="pre">slurm-core/series/</span></code>: overlays to define the OS of the Slurm components.</li>
</ul>
<p>For example, to deploy Slurm to a local LXD cloud, on Ubuntu Focal:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ juju deploy ./slurm-core/bundle.yaml <span class="se">\</span>
              --overlay ./slurm-core/clouds/lxd.yaml <span class="se">\</span>
              --overlay ./slurm-core/series/focal.yaml <span class="se">\</span>
</pre></div>
</div>
<p>Juju will then create the applications, configurations and LXD containers
described in the respective files, which will in turn define the contents of
the model.</p>
<p>It will take a moment get everything ready. You can check the status of your
model with juju's status:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ watch -n <span class="m">1</span> -c juju status --color

Model    Controller  Cloud/Region         Version  SLA          Timestamp
default  overlord    localhost/localhost  <span class="m">2</span>.8.7    unsupported  <span class="m">17</span>:44:29Z

App                 Version  Status  Scale  Charm               Store       Rev  OS      Notes
percona-cluster     <span class="m">5</span>.7.20   active      <span class="m">1</span>  percona-cluster     jujucharms  <span class="m">293</span>  ubuntu
slurm-configurator  <span class="m">20</span>.11.3  active      <span class="m">1</span>  slurm-configurator  <span class="nb">local</span>         <span class="m">1</span>  ubuntu
slurmctld           <span class="m">20</span>.11.3  active      <span class="m">1</span>  slurmctld           <span class="nb">local</span>         <span class="m">0</span>  ubuntu
slurmd              <span class="m">20</span>.11.3  active      <span class="m">1</span>  slurmd              <span class="nb">local</span>         <span class="m">0</span>  ubuntu
slurmdbd            <span class="m">20</span>.11.3  active      <span class="m">1</span>  slurmdbd            <span class="nb">local</span>         <span class="m">0</span>  ubuntu
slurmrestd          <span class="m">20</span>.11.3  active      <span class="m">1</span>  slurmrestd          <span class="nb">local</span>         <span class="m">0</span>  ubuntu

Unit                   Workload  Agent  Machine  Public address  Ports     Message
percona-cluster/0*     active    idle   <span class="m">0</span>        <span class="m">10</span>.34.166.18    <span class="m">3306</span>/tcp  Unit is ready
slurm-configurator/0*  active    idle   <span class="m">1</span>        <span class="m">10</span>.34.166.187             slurm-configurator available
slurmctld/0*           active    idle   <span class="m">2</span>        <span class="m">10</span>.34.166.222             slurmctld available
slurmd/0*              active    idle   <span class="m">3</span>        <span class="m">10</span>.34.166.219             slurmd available
slurmdbd/0*            active    idle   <span class="m">4</span>        <span class="m">10</span>.34.166.218             slurmdbd available
slurmrestd/0*          active    idle   <span class="m">5</span>        <span class="m">10</span>.34.166.66              slurm installed

Machine  State    DNS            Inst id        Series  AZ  Message
<span class="m">0</span>        started  <span class="m">10</span>.34.166.18   juju-01ab62-0  bionic      Running
<span class="m">1</span>        started  <span class="m">10</span>.34.166.187  juju-01ab62-1  focal       Running
<span class="m">2</span>        started  <span class="m">10</span>.34.166.222  juju-01ab62-2  focal       Running
<span class="m">3</span>        started  <span class="m">10</span>.34.166.219  juju-01ab62-3  focal       Running
<span class="m">4</span>        started  <span class="m">10</span>.34.166.218  juju-01ab62-4  focal       Running
<span class="m">5</span>        started  <span class="m">10</span>.34.166.66   juju-01ab62-5  focal       Running
</pre></div>
</div>
<p>Once the workload status is <em>active</em> and the agent status is <em>idle</em>, the Slurm
cluster is ready for use.</p>
<p>You can see the status of your cluster by running the <code class="docutils literal notranslate"><span class="pre">sinfo</span></code> command:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ juju run --unit slurm-configurator/0 sinfo
PARTITION         AVAIL  TIMELIMIT  NODES  STATE NODELIST
juju-compute-GsLk    up   infinite      <span class="m">1</span>   down juju-01ab62-3
configurator*     inact   infinite      <span class="m">1</span>   idle juju-01ab62-1
</pre></div>
</div>
<p>The nodes start in <em>down</em> state with a <code class="docutils literal notranslate"><span class="pre">Reason</span> <span class="pre">=</span> <span class="pre">New</span> <span class="pre">node</span></code>, so when you add
more nodes to the cluster, they will not execute the jobs from que queue. This
way it is possible to do some post installation before setting the nodes as
<em>idle</em>. You can double check that your nodes are down because of this and not
some other reason with <code class="docutils literal notranslate"><span class="pre">sinfo</span> <span class="pre">-R</span></code>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ juju run --unit slurm-configurator/0 <span class="s2">&quot;sinfo -R&quot;</span>
REASON               USER      TIMESTAMP           NODELIST
New node             root      <span class="m">2021</span>-03-09T20:24:09 ip-172-31-83-4
</pre></div>
</div>
<p>After setting the node up, to bring it back you need to run a Juju <em>action</em>:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ juju run-action slurmd/1 node-configured
$ juju run --unit slurm-configurator/0 sinfo
PARTITION         AVAIL  TIMELIMIT  NODES  STATE NODELIST
juju-compute-GsLk    up   infinite      <span class="m">1</span>   idle juju-01ab62-3
configurator*     inact   infinite      <span class="m">1</span>   idle juju-01ab62-1
</pre></div>
</div>
<p>You need to manually create an LXD image for CentOS7 in order to deploy it with
Juju.</p>
<p>First step is to download a configuration file describing the image:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ wget https://raw.githubusercontent.com/lxc/lxc-ci/master/images/centos.yaml
</pre></div>
</div>
<p>Juju needs two extra packages (<cite>sudo</cite> and <cite>openssh-server</cite>) that are not in the
base image. You need to manually add them in the <cite>packages</cite> section of the yaml
file. The first <cite>set</cite> of packages in the file should then be:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>packages:
  manager: yum
  update: <span class="nb">true</span>
  cleanup: <span class="nb">true</span>
  sets:
  - packages:
    - cronie
    - cronie-noanacron
    - curl
    - dhclient
    - initscripts
    - openssh-clients
    - passwd
    - policycoreutils
    - rootfiles
    - rsyslog
    - vim-minimal
    - sudo
    - openssh-server
    action: install
</pre></div>
</div>
<p>Now we need to install <cite>distrobuilder</cite> and generate the image:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ sudo snap install distrobuilder --classic
$ sudo distrobuilder build-lxd centos.yaml -o image.architecture<span class="o">=</span>x86_64 -o image.release<span class="o">=</span><span class="m">7</span> -o image.variant<span class="o">=</span>cloud
</pre></div>
</div>
<p>To make this new image available to Juju, we need to import it with an alias:</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>$ lxc image import lxd.tar.xz rootfs.squashfs --alias juju/centos7/amd64
</pre></div>
</div>
<p>You can check that the image was correctly imported to LXD with
<code class="docutils literal notranslate"><span class="pre">lxc</span> <span class="pre">image</span> <span class="pre">list</span></code>. To test it works with Juju, you can
<code class="docutils literal notranslate"><span class="pre">juju</span> <span class="pre">add-machine</span> <span class="pre">--series</span> <span class="pre">centos7</span></code>.</p>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">Omnivector Slurm Distribution</a></h1>








<h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Slurm Installation Overview</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#install-juju">Install Juju</a></li>
<li class="toctree-l2"><a class="reference internal" href="#setup-cloud">Setup cloud</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#aws">AWS</a></li>
<li class="toctree-l3"><a class="reference internal" href="#lxd">LXD</a></li>
<li class="toctree-l3"><a class="reference internal" href="#maas">MAAS</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#deploy-slurm">Deploy Slurm</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../architecture.html">OSD Architecture</a></li>
<li class="toctree-l1"><a class="reference internal" href="../operations/overview.html">Operations Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../configuration/configuration.html">Configuration and Administration</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="../index.html" title="previous chapter">Welcome to the documentation for the Omnivector Slurm Distribution!</a></li>
      <li>Next: <a href="../architecture.html" title="next chapter">OSD Architecture</a></li>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" />
      <input type="submit" value="Go" />
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2021, Omnivector Solutions.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 1.8.0</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="../_sources/slurm-installation/overview.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>